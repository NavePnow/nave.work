---
title: 痛苦的MARL(一)
date: 2019-07-04 20:00:02
tags: [RL, Python]
categories: [MARL]
thumbnail: https://images.unsplash.com/photo-1434030216411-0b793f4b4173?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=60
---

# 多智能体强化学习(一) 基础知识与博弈
<!--more-->
## 1. 引言
1. 多智能体通过和环境进行交互获取奖励值来学习改善自己的策略，其重点区别于单 agent 强化学习。
2. 算法收敛性与每个 agent 的最优策略相关，其最优策略又与空间中其他的 agent 相关联。
3. 联结动作:每个智能体当前动作组合而成的多智能体系统当前时刻的动作
$$
     A_t = [a_{1 ,t},a_{2 ,t},...,a_{n ,t}]^T 
$$
其中$a_{i,t}$表示第 i 个智能体在时刻 t选取的动作
## 2. 博弈论
由于多 agent 之间涉及到合作和竞争关系，引入博弈的概念。
### 1. 矩阵博弈
$$
    (n,A_1,A_2,...,A_n,R_1,R_2,...,R_n)
$$
其中 n 为 agent 数量，$A_i$为第 i 个 agent 的 action 集合，$R_i$为奖励函数，其值与空间中所有的 agent 的 action 集都有关系
    Target: 寻找纯策略或者混合策略 st 其收益虽大
#### 1. 纳什均衡
给定其他玩家继续采用纳什均衡策略而该玩家无法通过改变其自身策略获得更大回报的所有玩家策略的集合，即
$$
    V_i(\pi_1^*,...,\pi_i^*,...,\pi_n^*) \geq V_i(\pi_1^*,...,\pi_i,...,\pi_n^*)
$$
其中 $V_i(.)$ 表示玩家 i 在给定玩家策略下的期望回报，其含义为$\sum$用户 i 在联合动作下所获得回报乘以每个用户采用纳什均衡策略下选择该动作的概率，
$\pi_i$表示玩家 i 在策略空间$\prod_i$中选择的任一策略(可以理解为概率)
#### 2. 严格纳什均衡
大于等于公式严格成立
#### 3. 完全混合策略
Definition: The strategy that agent choose specific action based on possibity of all actions.
Possibities of all actions ara more than 0 percentage.
在猜硬币博弈中，只要用户 50%的概率选择正面，50%的概率选择反面，才能获得最大收益。如果按照纯策略一直选择正面，则对方会知道并选择让自己一直收益的情况（一直正面或反面），无法达到最大收益
#### 4. 纯策略
在囚徒困境博弈中，agent 在任何情况下都选择同样的行为，也就是向警官坦白，这时候无论对方怎么选择，自己都是 reward 和都是最大的。
### 2. agent在矩阵博弈中的纳什均衡
其奖励矩阵为
$$
    \begin{bmatrix}
    r_{11}&r_{12} \\ 
    r_{21}&r_{22} 
    \end{bmatrix}
$$
其中行表示行 agent，列表式列 agent，其下角标表示行列 agent 所采取的动作联结。
#### 5. 零和博弈
每玩一局游戏，都有一个玩家会赢而另一个玩家会输。两个玩家为完全竞争关系，
#### 6. 一般和博弈
任何类型的矩阵博弈
### 3. 多智能体强化学习策略
引入他妈的随机博弈：多智能体多个状态，是马尔科夫决策过程和矩阵博弈的过程。


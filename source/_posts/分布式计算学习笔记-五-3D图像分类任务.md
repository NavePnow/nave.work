---
title: 分布式计算学习笔记(五) 3D图像分类任务
date: 2020-02-18 19:22:27
thumbnail: https://images.unsplash.com/photo-1434030216411-0b793f4b4173?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=60
recommend: 0
top: 100
toc: true
categories: [Distributed System]
---

# 概述
本篇博客首先会对3D图像分类任务进行分析（因为我也不会），之后利用 Ray 的 API进行改写，完成分布式框架上的深度学习工作。
<!--more-->

## 3D图像数据表示

1. **点云（Point clouds）**: 3D空间点的集合，同时三维坐标(xyz)可以和其他属性(rgb)进行绑定进行点的属性的表示
2. ** 体素网格（Voxel grids）**: 在点云的基础上，将点变成具有固定大小和离散坐标的3D网络，有点类似于乐高。
3. ** 多边形网格（Polygon meshes）**: 3D物体表面存在有限个点的坐标并且相邻点之间都有线进行连接，从而形成了有一组与多边形表面近似，共享顶点的几何面组成。
4. ** 多视图表示（Multi-view representations）**: 顾名思义，具有多双目视觉的2D图像进行构建，从而形成真正的3D图像

## 3D图像分类任务

### 多视图输入学习（Learning with multi-view inputs）
[Multi-view CNN](https://arxiv.org/pdf/1505.00880.pdf)

首先从3D图中利用双目视觉获得多个2D渲染图像，每一个2D图像通过各自的CNN网络进行训练，之后进行汇总进行池化，之后再套CNN进行特征提取，最终输出物品的分类。

缺点可想而知，该方法无法真正以 3D 形式进行学习——固定数量的 2D 视图依旧只是一个对底层 3D 结构的不完美近似。

### 体积表示法学习（Learning with volumetric representations）
[VoxNet](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf)

和2D CNN网络结构差不多: 由2个卷积层，1个最大池化层和2个全连接层组成。首先给定一个输入-体素网格，利用占据栅格地图 (Occupancy Grid Map) 去表示每个体素在空间中的占有概率(Occupied状态的概率)，之后将修改后的数据喂入模型进行训练，输出类的分向量。

### 点云学习（Learning with point clouds）
[PointNet](https://arxiv.org/pdf/1612.00593.pdf)

# 实现
本文通过 CNN + MNIST 3D 图像进行分类任务，之后利用简单的 Ray Api 进行改写

(最开始的想法是利用keras高层api和dkeras(基于ray的分布式keras训练框架)，但是通过文档发现dkeras具有一定的局限性，只能使用固定的模型，所以就不再采用该方法进行实验)

## Dataset
该模型使用的数据集由原始的 MNIST 2D 图像通过点云的形式生成3D图像数据集，其数据文件格式为 HDF5。

其中HDF5文件是一种存放两类对象的容器：dataset 和 group。`dataset` 类似于数组的数据集，而 `group` 是类似于文件夹一样的容器。`group` 类比字典，`dataset` 类比numpy中的数组，`group` 内可以继续创建 `group` 和 `dataset` ，或者更直观的说，`group` 有点文件夹的感觉，而 `dataset` 就是文件夹中的文件。

### trainpointclouds.h5 & testpointclouds.h5
？ 点云的均值为零，最大维数范围为1 (The point clouds have zero mean and a maximum dimension range of 1)

数据集由多个 group 组成，每一个 gourp 是其一个数字的所有属性，含有：
- `"points" dataset`: 点云中每个点的3D坐标`x, y, z`
- `"normals" dataset`: 与每个点相关的法线的分量`nx, ny, nz`？(components of the unit normal associate to each point.)
- `"image" dataset`: 原始 MNIST 图像
- `"label" dataset`: 原始 MNIST label

### fulldatasetvectors.h5
该文件包含了从3D点云体素化后获得的4096维的数据和一些随机噪声点。其该文件共有4个group

    X_train = f["X_train"][:]()    # shape: (10000, 4096)
    y_train = f["y_train"][:]()    # shape: (10000,)   
    X_test  = f["X_test"][:]()     # shape: (2000, 4096) 
    y_test  = f["y_test"][:]()     # shape: (2000,)

点云中共有3个纬度，体素化过程中，每一维度划分成16个部分，所以在每个数字中，共有16^3=4096个体素块，通过计算每个体素块中点云数量的多少来描绘出每个数字的占据栅格地图

## CNN
从上一篇博客可知，对于2D图像的输入，CNN的输入为4D的tensor数据类型: `(batch_size, height, width, input_channels)` ，在该实验中，图片信息为3D，其CNN的输入变成了 `(batch_size, conv_dim1, conv_dim2, conv_dim3, input_channels) `。通过2D卷积操作可以得知，设置一个窗口，不断的在图片上进行扫描，每扫描一次，在高度维度上`(out_channel)` 上增加一层属性，或者说 `feature`，映射到 3D 图片，从2D扫描过渡到 3D 扫描，由原来的2D窗口变成了3D窗口。同时 `conv3d` 函数中的相关变量也需要做出调整，`filters` 表示的是输出空间的维度，可以理解为上文中的 `out_channel` , `kernel_size` 表示的是窗口的大小, 输出是一个三维的卷积空间，其他参数以及pool的使用和2D的差异不大，这里就不一一赘述。

## CNN + MNIST
[3d-cnn.py](https://github.com/NavePnow/CP3106-Independent-Project-NUS/blob/master/3d-cnn.py)
## CNN + Ray
[3d-cnn-ray](https://github.com/NavePnow/CP3106-Independent-Project-NUS/blob/master/3d-cnn-ray.py)

# Q&A

1. `from keras.models import Sequential`: Sequential 模型是一系列网络层按顺序构成的栈，是单输入和单输出的，层与层之间只有相邻关系的模型
2. `Dense(num, imput_dim = num)`:  等于 tf 中的全连阶层


# Reference
- [https://blog.csdn.net/lzc4869/article/details/94663616]()
- [https://blog.csdn.net/csdn15698845876/article/details/73278120]